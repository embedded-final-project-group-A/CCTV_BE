import torch
import numpy as np
from ultralytics import YOLO
import cv2
import os
import subprocess
from datetime import datetime, timedelta

# ë””ë²„ê¹… ì„¤ì •
DEBUG = False

def debug_log(*args, **kwargs):
    if DEBUG:
        print(*args, **kwargs)

# ëª¨ë¸ ë¡œë“œ
model = YOLO("./yolo/best.pt")
names = model.names

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs("output/captures", exist_ok=True)
os.makedirs("output/clips", exist_ok=True)

# ë¹„ë””ì˜¤ ê²½ë¡œ ì„¤ì •
video_path = "videos/yolo_test.mp4"
cap = cv2.VideoCapture(video_path)

# í”„ë ˆì„ ì •ë³´
fps = cap.get(cv2.CAP_PROP_FPS)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

# ë¶„ì„ êµ¬ê°„ ì„¤ì • (ì´ˆ ë‹¨ìœ„)
start_time_sec = 0
end_time_sec = 3

# ë¶„ì„ êµ¬ê°„ì˜ í”„ë ˆì„ ë²”ìœ„ ê³„ì‚°
start_frame = int(fps * start_time_sec)
end_frame = int(fps * end_time_sec)
if end_frame > total_frames:
    end_frame = total_frames

# í´ë¦½ ê¸¸ì´ ì„¤ì •
clip_duration = 2.0
half_clip_frames = int(fps * (clip_duration / 2))

# ì˜ìƒ ì‹œì‘ ì‹œê°„
video_start_time = datetime(2025, 5, 27, 12, 0, 0)

# ìƒíƒœ ë³€ìˆ˜
frame_count = 0
helmet_capture_count = 0
frames_buffer = []
buffer_start_frame_idx = 0

# ì´ë²¤íŠ¸ ë¡œê·¸
event_logs = []

# ğŸ”§ ffmpegë¥¼ ì´ìš©í•œ H.264 ì¸ì½”ë”© ë³€í™˜ í•¨ìˆ˜
def convert_to_h264(input_path, output_path):
    cmd = [
        "ffmpeg", "-y", "-i", input_path,
        "-vcodec", "libx264",
        "-profile:v", "baseline",
        "-level", "3.0",
        "-pix_fmt", "yuv420p",
        "-acodec", "aac",
        "-strict", "experimental",
        output_path
    ]
    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)


# í´ë¦½ ì €ì¥ í•¨ìˆ˜ (OpenCV + ffmpeg ì¬ì¸ì½”ë”©)
def save_clip(buffer, fps, output_path_base):
    if not buffer:
        return False, None
    height, width = buffer[0].shape[:2]
    temp_path = output_path_base + "_raw.mp4"

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(temp_path, fourcc, fps, (width, height))
    for f in buffer:
        out.write(f)
    out.release()

    final_path = output_path_base + ".mp4"
    convert_to_h264(temp_path, final_path)
    os.remove(temp_path)  # ì„ì‹œíŒŒì¼ ì œê±°

    return os.path.exists(final_path), final_path

# íƒì§€ ê²°ê³¼ ì²˜ë¦¬ í•¨ìˆ˜
def use_result(frame, results, current_idx):
    global helmet_capture_count
    captured = False

    if results and results[0].boxes is not None:
        bboxes = results[0].boxes.xyxy.cpu().numpy().astype(int)
        classes = results[0].boxes.cls.cpu().numpy().astype(int)
        pred_box = zip(classes, bboxes)

        for cls, bbox in pred_box:
            (x1, y1, x2, y2) = bbox
            label = names.get(cls, str(cls))
            debug_log(f"íƒì§€: í´ë˜ìŠ¤={label}, ìœ„ì¹˜=({x1},{y1})~({x2},{y2})")

            if label.lower() == "helmet" and not captured:
                img_path = f"output/captures/helmet_capture_{helmet_capture_count}.jpg"
                img_saved = cv2.imwrite(img_path, frame)

                start = max(0, current_idx - half_clip_frames)
                end = min(len(frames_buffer), current_idx + half_clip_frames)
                clip_frames = frames_buffer[start:end]
                clip_base = f"output/clips/helmet_clip_{helmet_capture_count}"
                clip_saved, clip_path = save_clip(clip_frames, fps, clip_base)

                if img_saved and clip_saved:
                    debug_log(f"[âœ… ì´ë¯¸ì§€ ì €ì¥ë¨] {img_path}")
                    debug_log(f"[ğŸï¸ í´ë¦½ ì €ì¥ë¨] {clip_path}")

                    event_time = video_start_time + timedelta(seconds=frame_count / fps)
                    event_time_str = event_time.strftime("%Y-%m-%dT%H:%M:%S")

                    image_url = f"https://localhost:8000/{img_path}"
                    clip_url = f"https://localhost:8000/{clip_path}"

                    helmet_capture_count += 1
                    captured = True

                    return (event_time_str, image_url, clip_url)
                else:
                    debug_log(f"[âŒ ì €ì¥ ì‹¤íŒ¨] ì´ë¯¸ì§€: {img_saved}, í´ë¦½: {clip_saved}")

    return None

# ë©”ì¸ ë£¨í”„
while True:
    ret, frame = cap.read()
    if not ret:
        debug_log("ì˜ìƒ ë")
        break

    if frame_count < start_frame:
        frame_count += 1
        continue

    if frame_count > end_frame:
        debug_log("ë¶„ì„ ì¢…ë£Œ ì‹œê°„ ë„ë‹¬")
        break

    frames_buffer.append(frame.copy())

    max_buffer_len = int(fps * 10)
    if len(frames_buffer) > max_buffer_len:
        frames_buffer.pop(0)
        buffer_start_frame_idx += 1

    results = model(frame, verbose=False)
    current_buffer_idx = frame_count - buffer_start_frame_idx

    event_log = use_result(frame, results, current_buffer_idx)
    if event_log is not None:
        event_logs.append(event_log)
        print(f"[ì´ë²¤íŠ¸ ë¡œê·¸] Time: {event_log[0]}, Image URL: {event_log[1]}, Clip URL: {event_log[2]}")

    frame_count += 1

    if cv2.waitKey(1) == 27:
        break

cap.release()
cv2.destroyAllWindows()

print(f"ì´ {helmet_capture_count}ê°œì˜ í—¬ë©§ íƒì§€ ì´ë¯¸ì§€ì™€ í´ë¦½ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
